{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of DICOM Images and Labels for Neural Network Training with Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking 2D Images into 3D and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from monai.data import Dataset\n",
    "from monai.transforms import LoadImage\n",
    "\n",
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        # the self.patients list is filled with the names of directories in image_dir when then directory name includes \"PANCREAS\"\n",
    "        self.patients = [f for f in sorted(os.listdir(image_dir)) if \"PANCREAS\" in f]\n",
    "        self.max_depth = self.calculate_max_depth()\n",
    "\n",
    "    def calculate_max_depth(self):\n",
    "        max_depth = 0\n",
    "        for patient_folder in self.patients:\n",
    "            patient_image_dir = os.path.join(self.image_dir, patient_folder)\n",
    "            volume = self.load_dicom_volume(patient_image_dir) # finds the number of slices in the folder\n",
    "            if volume.shape[0] > max_depth:\n",
    "                max_depth = volume.shape[0]\n",
    "        return max_depth\n",
    "\n",
    "    def load_dicom_volume(self, patient_image_dir):\n",
    "        subfolder = next(os.walk(patient_image_dir))[1][0]\n",
    "        deepest_folder = next(os.walk(os.path.join(patient_image_dir, subfolder)))[1][0]\n",
    "        final_path = os.path.join(patient_image_dir, subfolder, deepest_folder)\n",
    "        files = sorted(os.listdir(final_path), key=lambda x: pydicom.dcmread(os.path.join(final_path, x)).InstanceNumber)\n",
    "        # os.listdir(final_path): refers to the folder with the dicom files\n",
    "        # sorted() usually sorts them in alphabetical, but using the key parameter makes sure a specific order is used\n",
    "        # key=lambda x: pydicom.dcmread(os.path.join(final_path, x)).InstanceNumber:\n",
    "            # lambda x: defines an anonymous function where x is the name of the variable that represents each element in the list\n",
    "            # pydicom.dcmread(os.path.join(final_path, x)).InstanceNumber : the instance number of each dicom image is used to ensure that the images are sorted in consecutive, sequential order.\n",
    "\n",
    "        # stacks all of the 2D slices for each patient into a 3D array\n",
    "        volume = np.stack([pydicom.dcmread(os.path.join(final_path, f)).pixel_array for f in files])\n",
    "\n",
    "        # converts the numpy array into a torch tensor\n",
    "        return torch.from_numpy(volume).float()  # No need to add batch dimension manually\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_folder = self.patients[idx]\n",
    "        patient_image_dir = os.path.join(self.image_dir, patient_folder)\n",
    "        patient_label_path = os.path.join(self.label_dir, f\"label{patient_folder.split('_')[-1]}.nii.gz\")\n",
    "        \n",
    "        volume = self.load_dicom_volume(patient_image_dir)\n",
    "\n",
    "        # label image is loaded with MONAI's LoadImage where only the image is loaded with image_only = True\n",
    "        label = LoadImage(image_only=True)(patient_label_path)\n",
    "        # rearranges the axes of the label array to match the conventional layout expected by PyTorch models (num_channels x height x width)\n",
    "        label = np.transpose(label, (2, 0, 1))\n",
    "        # turns numpy array into torch tensor\n",
    "        label = torch.from_numpy(label).float()\n",
    "\n",
    "        # Padding to maximum depth for depth (slice axis)\n",
    "        pad_size = self.max_depth - volume.shape[0]\n",
    "        volume = torch.nn.functional.pad(volume, (0, 0, 0, 0, 0, pad_size))\n",
    "        label = torch.nn.functional.pad(label, (0, 0, 0, 0, 0, pad_size))\n",
    "\n",
    "        # Correcting shape to (512, 512, padded_num_slices, 1)\n",
    "        volume = volume.permute(2, 1, 0).unsqueeze(-1)  # changes the shape of the volume tensor to fit the format expected by neural networks of (H, W, D, C)\n",
    "        # permute function allows for changing dimensions of the tensor\n",
    "        # unsqueeze function adds a new dimension at the specified index. Here the index is -1 which indicates that the new dimension added into the last position. \n",
    "        label = label.permute(2, 1, 0).unsqueeze(-1)    # changes the shape of the volume tensor to fit the format expected by neural networks of (H, W, D, C)\n",
    "\n",
    "        if self.transforms:\n",
    "            volume = self.transforms(volume)\n",
    "            label = self.transforms(label)\n",
    "\n",
    "        return volume.squeeze(0), label.squeeze(0)  # Ensure removing any singleton dimension at batch axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up and testing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from monai.transforms import Compose, ScaleIntensity, EnsureType\n",
    "from monai.transforms import LoadImage\n",
    "\n",
    "# Transforms and DataLoader\n",
    "transforms = Compose([\n",
    "    ScaleIntensity(), # normalizes or scales image intensities\n",
    "    EnsureType(dtype=torch.float32) # ensures that the data type of the tensors after transformationis torch.float32 which is standard for PyTorch models\n",
    "])\n",
    "\n",
    "# Paths\n",
    "image_root_dir = \"/Users/asmit/Programming/SDS323/Final_Project/manifest-1599750808610/Pancreas-CT\"\n",
    "label_root_dir = \"/Users/asmit/Programming/SDS323/Final_Project/TCIA_pancreas_labels-02-05-2017\"\n",
    "\n",
    "dataset = MedicalImageDataset(image_root_dir, label_root_dir, transforms=transforms)\n",
    "\n",
    "# DataLoader is initialized which is standard for PyTorch usage\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "# note that the dataset has volume, label at this point\n",
    "# batch_size = 1 ensures that each item retried by the DataLoader will contain one sample from the dataset which in this case is one volume and one label\n",
    "\n",
    "# Test DataLoader\n",
    "for volume, label in dataloader:\n",
    "    # Squeeze out the batching dimension which is the first dimension\n",
    "    volume, label = volume.squeeze(0), label.squeeze(0) # Comment if permute block is commented above\n",
    "    print(\"Volume shape:\", volume.shape, \"Label shape:\", label.shape)\n",
    "    break # Checking only the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Exploratory Data Analysis\n",
    "##### Initial General Notes: There is an imbalance of classes in the data as there are multiple organs in the image of the abdomen other than the pancreas. There are no missing data as the image to label ratio is clearly 1:1.\n",
    "#### Pixel Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_intensity_distribution(dataloader, num_batches=1, samples_per_volume=100000):\n",
    "    volume_intensities = []\n",
    "    label_intensities = []\n",
    "\n",
    "    for i, (volume, label) in enumerate(dataloader):\n",
    "\n",
    "        # Stopping criteria\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "        # Squeeze out channel and batch singleton dimensions\n",
    "        volume = volume.squeeze().numpy()\n",
    "        label = label.squeeze().numpy()\n",
    "\n",
    "        # Iterate through each slice in batch\n",
    "        for vol_slice, lbl_slice in zip(volume, label):\n",
    "\n",
    "            # Filter out padding values\n",
    "            non_padded_pixels = vol_slice[vol_slice > 0]  # Adjust this threshold as needed\n",
    "            labeled_pixels = vol_slice[lbl_slice > 0]\n",
    "\n",
    "            # Randomly sample pixels to reduce the data size\n",
    "            if len(non_padded_pixels) > samples_per_volume:\n",
    "                sampled_volume_pixels = np.random.choice(non_padded_pixels, samples_per_volume, replace=False)\n",
    "            else:\n",
    "                sampled_volume_pixels = non_padded_pixels\n",
    "            \n",
    "            if len(labeled_pixels) > samples_per_volume:\n",
    "                sampled_label_pixels = np.random.choice(labeled_pixels, samples_per_volume, replace=False)\n",
    "            else:\n",
    "                sampled_label_pixels = labeled_pixels\n",
    "\n",
    "            volume_intensities.extend(sampled_volume_pixels)\n",
    "            label_intensities.extend(sampled_label_pixels)\n",
    "    \n",
    "    # Plot the histogram of the sampled intensities\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    axs[0].hist(volume_intensities, bins=256, color='skyblue', alpha=0.75)  # Adjust bins if needed\n",
    "    axs[0].set_title('Pixel Intensity Distribution of Volumes')\n",
    "    axs[0].set_xlabel('Pixel Intensity')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    axs[1].hist(label_intensities, bins=30, color='lightcoral', alpha=0.75)  # Adjust bins if needed\n",
    "    axs[1].set_title('Pixel Intensity Distribution of Labeled Pancreas')\n",
    "    axs[1].set_xlabel('Pixel Intensity')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the aggregate dataloader from memory\n",
    "del dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def create_data_loaders(dataset, batch_size=1):\n",
    "\n",
    "    # Determine sizes of each split\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.6 * total_size)\n",
    "    val_size = int(0.2 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Randomly split the dataset into train, cv, and test sets\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(832) # Set seed\n",
    "    )\n",
    "\n",
    "    # Create data loaders for each split\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create dataloaders\n",
    "dataset = MedicalImageDataset(image_root_dir, label_root_dir, transforms=transforms)\n",
    "train_loader, val_loader, test_loader = create_data_loaders(dataset, batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
