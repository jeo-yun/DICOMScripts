{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of DICOM Images and Labels for Neural Network Training with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from monai.data import Dataset\n",
    "from monai.transforms import LoadImage\n",
    "\n",
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        # the self.patients list is filled with the names of directories in image_dir when then directory name includes \"PANCREAS\"\n",
    "        self.patients = [f for f in sorted(os.listdir(image_dir)) if \"PANCREAS\" in f]\n",
    "        self.max_depth = self.calculate_max_depth()\n",
    "\n",
    "    def calculate_max_depth(self):\n",
    "        max_depth = 0\n",
    "        for patient_folder in self.patients:\n",
    "            patient_image_dir = os.path.join(self.image_dir, patient_folder)\n",
    "            volume = self.load_dicom_volume(patient_image_dir) # finds the number of slices in the folder\n",
    "            if volume.shape[0] > max_depth:\n",
    "                max_depth = volume.shape[0]\n",
    "        return max_depth\n",
    "\n",
    "    def load_dicom_volume(self, patient_image_dir):\n",
    "        subfolder = next(os.walk(patient_image_dir))[1][0]\n",
    "        deepest_folder = next(os.walk(os.path.join(patient_image_dir, subfolder)))[1][0]\n",
    "        final_path = os.path.join(patient_image_dir, subfolder, deepest_folder)\n",
    "        files = sorted(os.listdir(final_path), key=lambda x: pydicom.dcmread(os.path.join(final_path, x)).InstanceNumber)\n",
    "        # os.listdir(final_path): refers to the folder with the dicom files\n",
    "        # sorted() usually sorts them in alphabetical, but using the key parameter makes sure a specific order is used\n",
    "        # key=lambda x: pydicom.dcmread(os.path.join(final_path, x)).InstanceNumber:\n",
    "            # lambda x: defines an anonymous function where x is the name of the variable that represents each element in the list\n",
    "            # pydicom.dcmread(os.path.join(final_path, x)).InstanceNumber : the instance number of each dicom image is used to ensure that the images are sorted in consecutive, sequential order.\n",
    "\n",
    "        # stacks all of the 2D slices for each patient into a 3D array\n",
    "        volume = np.stack([pydicom.dcmread(os.path.join(final_path, f)).pixel_array for f in files])\n",
    "\n",
    "        # converts the numpy array into a torch tensor\n",
    "        return torch.from_numpy(volume).float()  # No need to add batch dimension manually\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_folder = self.patients[idx]\n",
    "        patient_image_dir = os.path.join(self.image_dir, patient_folder)\n",
    "        patient_label_path = os.path.join(self.label_dir, f\"label{patient_folder.split('_')[-1]}.nii.gz\")\n",
    "        \n",
    "        volume = self.load_dicom_volume(patient_image_dir)\n",
    "\n",
    "        # label image is loaded with MONAI's LoadImage where only the image is loaded with image_only = True\n",
    "        label = LoadImage(image_only=True)(patient_label_path)\n",
    "        # rearranges the axes of the label array to match the conventional layout expected by PyTorch models (num_channels x height x width)\n",
    "        label = np.transpose(label, (2, 0, 1))\n",
    "        # turns numpy array into torch tensor\n",
    "        label = torch.from_numpy(label).float()\n",
    "\n",
    "        # Padding to maximum depth for depth (slice axis)\n",
    "        pad_size = self.max_depth - volume.shape[0]\n",
    "        volume = torch.nn.functional.pad(volume, (0, 0, 0, 0, 0, pad_size))\n",
    "        label = torch.nn.functional.pad(label, (0, 0, 0, 0, 0, pad_size))\n",
    "\n",
    "        # Correcting shape to (512, 512, padded_num_slices, 1)\n",
    "        volume = volume.permute(2, 1, 0).unsqueeze(-1)  # changes the shape of the volume tensor to fit the format expected by neural networks of (H, W, D, C)\n",
    "        # permute function allows for changing dimensions of the tensor\n",
    "        # unsqueeze function adds a new dimension at the specified index. Here the index is -1 which indicates that the new dimension added into the last position. \n",
    "        label = label.permute(2, 1, 0).unsqueeze(-1)    # changes the shape of the volume tensor to fit the format expected by neural networks of (H, W, D, C)\n",
    "\n",
    "        if self.transforms:\n",
    "            volume = self.transforms(volume)\n",
    "            label = self.transforms(label)\n",
    "\n",
    "        return volume.squeeze(0), label.squeeze(0)  # Ensure removing any singleton dimension at batch axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from monai.transforms import Compose, ScaleIntensity, EnsureType\n",
    "from monai.transforms import LoadImage\n",
    "\n",
    "# Transforms and DataLoader\n",
    "transforms = Compose([\n",
    "    ScaleIntensity(), # normalizes or scales image intensities\n",
    "    EnsureType(dtype=torch.float32) # ensures that the data type of the tensors after transformationis torch.float32 which is standard for PyTorch models\n",
    "])\n",
    "\n",
    "# Paths\n",
    "image_root_dir = \"/Users/asmit/Programming/SDS323/Final_Project/manifest-1599750808610/Pancreas-CT\"\n",
    "label_root_dir = \"/Users/asmit/Programming/SDS323/Final_Project/TCIA_pancreas_labels-02-05-2017\"\n",
    "\n",
    "dataset = MedicalImageDataset(image_root_dir, label_root_dir, transforms=transforms)\n",
    "\n",
    "# DataLoader is initialized which is standard for PyTorch usage\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "# note that the dataset has volume, label at this point\n",
    "# batch_size = 1 ensures that each item retried by the DataLoader will contain one sample from the dataset which in this case is one volume and one label\n",
    "\n",
    "# Test DataLoader\n",
    "for volume, label in dataloader:\n",
    "    # Squeeze out the batching dimension which is the first dimension\n",
    "    volume, label = volume.squeeze(0), label.squeeze(0) # Comment if permute block is commented above\n",
    "    print(\"Volume shape:\", volume.shape, \"Label shape:\", label.shape)\n",
    "    break # Checking only the first batch"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
